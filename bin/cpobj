#!/usr/bin/env python

import os
import sys
import math
import re
import logging
import boto.s3
from boto.s3.connection import S3Connection
from boto.s3.connection import OrdinaryCallingFormat
import progressbar
import mimetypes
import threading
import Queue
import time
from filechunkio import FileChunkIO

## Load our local library functions
sys.path.insert(0, "%s/../lib" % os.path.dirname(sys.argv[0]))
from umobj_utils import umobj_logging, umobj_init_keyboard_interrupt
from umobj_key import create_directory

pbar = None


def usage():
    prog_name = os.path.basename(sys.argv[0])
    usage_string = """
NAME
    %s - Copy files to and from a S3 compatiable object store

SYNOPSIS
    %s [OPTION]... SRC [SRC]+ BUCKET[:DEST]

    %s [OPTION]... BUCKET[:SRC] DEST

OPTIONS SUMMARY

    -a, --access_key <access_key>  Object store access key
    -s, --secret_key <secret_key>  Object store secret key
    -S, --server <server>          Object store server name
    -P, --port <port>              Object store server port [Default: 443]
    -m, --multipart                Instantiate multipart uploads [Default: no]
    -r, --recursive                Copy files recursively [Default: no]
    -f, --force                    Force overwrite files in download mode
    -V, --verbose                  Verbose mode
    -D, --debug                    Debug mode
    -h, --help

OPTIONS
    access_key  - Your Access Key ID.  If not supplied, boto will
                  use the value of the environment variable OBJ_ACCESS_KEY_ID.
    secret_key  - Your Secret Access Key.  If not supplied, will use the value
                  of the environment variable OBJ_SECRET_ACCESS_KEY.
    server      - The object storage server you want to connect to.  This can
                  be overridden by the OBJ_SERVER environment variable.
    BUCKET      - The bucket that is to be used for this copy operation.  This
                  is necessary on one and only one of the SRC and DEST
                  arguments.
    SRC         - Local file system directory/file or object store key.  A
                  directory is allowed for uploads if recursive is turned on,
                  and is allowed for download.
    DEST        - Local file system directory/file or object store key.  A
                  directory is allowed for downloads.

""" % (prog_name, prog_name, prog_name)
    print usage_string
    sys.exit()


def transfer_stats(trans_bytes, total_bytes):
    try:
        pbar.update(trans_bytes)
    except AssertionError, e:
        print e


def upload_file(key, filename, progress=True):
    global pbar
    if os.path.islink(filename):
        logging.warning('Skipping %s, symlink.' % filename)
        return -1
    if not os.path.isfile(filename):
        logging.warning('Skipping %s, unknown file type.' % filename)
        return -1
    file_size = os.stat(filename).st_size
    logging.info("Uploading %s with %d bytes." % (filename, file_size))
    if file_size is 0:
        if progress:
            pbar = progressbar.ProgressBar(maxval=100)
            pbar.start()
        key.set_contents_from_filename(filename)
        if progress:
            pbar.update(100)
            pbar.finish()
        return 0
    if progress:
        pbar = progressbar.ProgressBar(maxval=file_size)
        pbar.start()
    try:
        if progress:
            key.set_contents_from_filename(filename, cb=transfer_stats,
                                           num_cb=100)
        else:
            key.set_contents_from_filename(filename)
    except IOError, e:
        print e
        return 0
    if progress:
        pbar.finish()
    return file_size


def download_file(key, filename, progress=True):
    logging.info("Downloading %s with %d bytes." % (filename, key.size))
    global pbar
    if key.size is 0:
        pbar = progressbar.ProgressBar(maxval=100)
        pbar.start()
        if filename.endswith('/'):
            if not os.path.isdir(filename):
                logging.info("Creating directory %s." % filename)
                os.makedirs(filename)
            else:
                logging.info("Directory %s already exists, skipping." %
                             filename)
        else:
            key.get_contents_to_filename(filename)
        pbar.update(100)
        pbar.finish()
        return
    pbar = progressbar.ProgressBar(maxval=key.size)
    pbar.start()
    if os.path.isdir(filename):
        filename = filename + os.sep + os.path.basename(key.name)
    f = open(filename, 'w')
    key.get_contents_to_file(f, cb=transfer_stats, num_cb=100)
    f.close()
    pbar.finish()


class UploadThread(threading.Thread):
    def __init__(self, mp, queue):
        threading.Thread.__init__(self)
        self.mp = mp
        self.queue = queue
        self.obj = self.mp.connect()

    def run(self):
        while True:
            try:
                part_num, offset, bytes = self.queue.get(True, 2)
                logging.info('Starting part %d on offset %d with %d bytes.' %
                             (part_num, offset, bytes))
            except Queue.Empty:
                return
            self._upload_part(part_num, offset, bytes)
            self.queue.task_done()

    def _upload_part(self, part_num, offset, bytes, retries=10):
        try:
            bucket = self.obj.get_bucket(self.mp.bucketname)
            for mp in bucket.get_all_multipart_uploads():
                if mp.id == self.mp.mp_id:
                    logging.debug('%s : Uploading chunk (%d) %s' %
                                  (self.mp.mp_id, retries, part_num))
                    with FileChunkIO(self.mp.filename, 'r', offset=offset,
                                     bytes=bytes) as fp:
                        mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
        except Exception, exc:
            print exc
            if retries:
                self._upload_part(part_num, offset,
                                  bytes, retries=retries-1)
            else:
                logging.error('%s : Failed uploading part %d' %
                              (self.mp.mp_id, part_num))
                raise exc
        else:
            logging.info('%s : Uploaded part %d' % (self.mp.mp_id, part_num))


class DownloadThread(threading.Thread):
    def __init__(self, mp, queue):
        threading.Thread.__init__(self)
        self.mp = mp
        self.queue = queue
        self.obj = self.mp.connect()

    def run(self):
        while True:
            try:
                start_byte, end_byte = self.queue.get(True, 2)
                logging.info('Starting download bytes %d - %d.' %
                             (start_byte, end_byte))
            except Queue.Empty:
                return
            self._download_part(start_byte, end_byte)
            self.queue.task_done()

    def _download_part(self, start_byte, end_byte):
        try:
            key_range = self.obj.make_request(
                'GET',
                bucket=self.mp.bucketname,
                key=self.mp.keyname,
                headers={'Range': "bytes=%d-%d" % (start_byte, end_byte)})
            filename = self.mp.filename
            fd = os.open(filename, os.O_WRONLY)
            logging.debug("Opening file descriptor %d, seeking to %d" %
                         (fd, start_byte))
            os.lseek(fd, start_byte, os.SEEK_SET)
            chunk_size = min((end_byte-start_byte), 32 * 1024 * 1024)
            while True:
                data = key_range.read(chunk_size)
                if data == "":
                    break
                os.write(fd, data)
        except Exception, exc:
            print exc


class MultiPart:

    ### code adapted from https://gist.github.com/fabiant7t/924094
    def __init__(self, access_key, secret_key, server='obj.umiacs.umd.edu',
                 port=443, secure=True):
        self.access_key = access_key
        self.secret_key = secret_key
        self.server = server
        self.port = port
        self.secure = secure
        self.mp_id = None
        self.bucketname = None
        self.filename = None
        self.keyname = None

    def connect(self):
        return S3Connection(host=self.server,
                            port=self.port,
                            is_secure=self.secure,
                            aws_access_key_id=self.access_key,
                            aws_secret_access_key=self.secret_key,
                            calling_format=OrdinaryCallingFormat())

    def start_download(self, bucketname, keyname, filename, threads=4):
        logging.info("Starting a multipart downlaod.")
        self.bucketname = bucketname
        if os.path.isdir(filename):
            self.filename = filename + os.sep + os.path.basename(keyname)
        else:
            self.filename = filename
        self.keyname = keyname
        obj = self.connect()
        bucket = obj.get_bucket(self.bucketname)
        key = bucket.get_key(keyname)
        size = key.size
        if size < 5 * 1024 * 1024:
            # don't multipart < 5MB files
            download_file(key, filename)
            return
        else:
            # create the file
            fd = os.open(self.filename, os.O_CREAT)
            os.close(fd)
            bytes_per_chunk = max(int(math.sqrt(5242880) * math.sqrt(size)),
                                  5242880)
            chunk_amount = int(math.ceil(size / float(bytes_per_chunk)))
            queue = Queue.Queue()
            for i in range(chunk_amount):
                offset = i * bytes_per_chunk
                remaining_bytes = size - offset
                bytes = min([bytes_per_chunk, remaining_bytes])
                queue.put((offset, offset + bytes - 1))
            for i in range(threads):
                t = DownloadThread(self, queue)
                t.setDaemon(True)
                t.start()
            queue.join()

    def start_upload(self, bucketname, keyname, filename, policy, threads=4):
        self.bucketname = bucketname
        self.filename = filename
        headers = {}
        obj = self.connect()
        bucket = obj.get_bucket(self.bucketname)
        mtype = mimetypes.guess_type(keyname)[0] or 'application/octet-stream'
        headers.update({'Content-Type': mtype})
        mp = bucket.initiate_multipart_upload(keyname, headers=headers)
        self.mp_id = mp.id
        logging.info("%s: Starting a mp upload for %s" % (mp.id, filename))
        source_size = os.stat(filename).st_size
        bytes_per_chunk = max(int(math.sqrt(5242880) * math.sqrt(source_size)),
                              5242880)
        chunk_amount = int(math.ceil(source_size / float(bytes_per_chunk)))
        logging.info("%s : Size: %16d   Chunk Size: %16d   Number Chunks: %8d" %
                     (mp.id, source_size, bytes_per_chunk, chunk_amount))
        queue = Queue.Queue()
        logging.info("%s : Starting a pool with %d threads." %
                     (mp.id, threads))

        ### putting all the tasks together in a queue
        for i in range(chunk_amount):
            offset = i * bytes_per_chunk
            remaining_bytes = source_size - offset
            bytes = min([bytes_per_chunk, remaining_bytes])
            part_num = i + 1
            queue.put((part_num, offset, bytes))

        for i in range(threads):
            t = UploadThread(self, queue)
            t.setDaemon(True)
            t.start()

        pbar = progressbar.ProgressBar(maxval=chunk_amount)
        pbar.start()
        while not queue.empty():
            pbar.update(chunk_amount - queue.qsize())
            time.sleep(1)
        pbar.finish()
        queue.join()

        if len(mp.get_all_parts()) == chunk_amount:
            mp.complete_upload()
            key = bucket.get_key(keyname)
            logging.debug("%s : Applying bucket policy %s" % (mp.id, policy))
            key.set_acl(policy)
        else:
            logging.warning("%s : Canceling mulitpart upload." % mp.id)
            mp.cancel_upload()


def cpobj_download(access_key, secret_key, bucket_name, dest, key_name, recursive=False, multi=False):
    bucket = obj.get_bucket(bucket_name)
    if recursive:
        logging.info("Starting recursive download %s to %s prefix %s" %
                     (bucket.name, dest, key_name))
        if not os.path.isdir(dest):
            logging.error("DEST %s is not a directory." % dest)
            usage()
        else:
            for key in bucket.list(prefix=key_name):
                logging.info("Downloading key %s" % key)
                download_file(key, dest.rstrip(os.sep) + os.sep + key.name)
    else:
        if not key_name:
            logging.error("Must specify a key to download or use recusive option")
            usage()
        if os.path.isfile(dest) and not force:
            logging.error("File %s already exists " % dest +
                          "please force flag to overwrite.")
            usage()
        else:
            if multi:
                m = MultiPart(access_key, secret_key, server, port)
                m.start_download(bucket_name, key_name, dest)
            else:
                key = bucket.get_key(key_name)
                download_file(key, dest)


def cpobj_upload(access_key, secret_key, bucket_name, src, dest_name, recursive=False, multi=False):
    bucket = obj.get_bucket(bucket_name)
    size = os.stat(src).st_size
    policy = bucket.get_acl()
    if recursive and os.path.isdir(src):
        if dest_name:
            prefix = dest_name.rstrip(os.sep) + '/'
            logging.info("Create directory %s" % prefix)
            dir_key = bucket.new_key(prefix)
            create_directory(dir_key)
            logging.debug("Applying bucket policy %s" % policy)
            dir_key.set_acl(policy)
        else:
            prefix = ''
        for root, dirs, files in os.walk(src.rstrip(os.sep)):
            directory = prefix + root
            logging.info("Create directory %s" % directory)
            dir_key = bucket.new_key(directory + '/')
            create_directory(dir_key)
            logging.debug("Applying bucket policy %s" % policy)
            dir_key.set_acl(policy)
            for f in files:
                filename = root + os.sep + f
                size = os.stat(filename).st_size
                keyname = prefix + root + '/' + f
                logging.info("Upload key %s from file %s" %
                             (keyname, filename))
                if multi or (size > (1024*1024*1024)):
                    m = MultiPart(access_key, secret_key, server, port)
                    m.start_upload(bucket_name, keyname, filename, policy)
                else:
                    key = bucket.new_key(keyname)
                    res = upload_file(key, filename)
                    if res >= 0: 
                        logging.debug("Applying bucket policy %s" % policy)
                        key.set_acl(policy)
    else:
        if os.path.isdir(src):
            logging.warning("Skipping directory %s, use the recursive option." % src)
            return
        if not os.path.isfile(src):
            logging.error("File %s does not exist." % src)
            return
        if dest_name:
            current_path = ''
            for dir_part in dest_name.lstrip(os.sep).split(os.sep):
                current_path = current_path + dir_part + '/'
                key = bucket.new_key(current_path)
                create_directory(key)
                logging.debug("Applying bucket policy %s" % policy)
                key.set_acl(policy)
                key_name = current_path + os.path.basename(src)
        else:
            key_name = os.path.basename(src)
        if multi or (size > (1024*1024*1024)):
            logging.info("Starting a multipart upload.")
            m = MultiPart(access_key, secret_key, server, port)
            m.start_upload(bucket_name, key_name, src, policy)
        else:
            key = bucket.new_key(key_name)
            res = upload_file(key, src)
            if res >= 0:
                logging.debug("Applying bucket policy %s" % policy)
                key.set_acl(policy)


if __name__ == "__main__":
    import getopt

    umobj_init_keyboard_interrupt()

    try:
        opts, args = getopt.getopt(sys.argv[1:],
                                   'a:b:s:S:P:hmrfVD',
                                   ['access_key=', 'secret_key=',
                                    'server=', 'port=', 'multipart',
                                    'help', 'recursive', 'force',
                                    'verbose', 'debug'])
    except getopt.GetoptError, err:
        print str(err)  # will print something like "option -a not recognized"
        usage()

    force = False
    multi = False
    level = logging.WARNING
    access_key = None
    secret_key = None
    bucket_name = None
    try:
        server = os.environ['OBJ_SERVER']
    except:
        server = 'obj.umiacs.umd.edu'
    port = 443
    recursive = False

    for o, a in opts:
        if o in ('-h', '--help'):
            usage()
            sys.exit()
        if o in ('-a', '--access_key'):
            access_key = a
        if o in ('-s', '--secret_key'):
            secret_key = a
        if o in ('-b', '--bucket'):
            bucket_name = a
        if o in ('-S', '--server'):
            server = a
        if o in ('-P', '--port'):
            port = a
        if o in ('-m', '--multipart'):
            multi = True
        if o in ('-r', '--recursive'):
            recursive = True
        if o in ('-f', '--force'):
            force = True
        if o in ('-V', '--verbose'):
            level = logging.INFO
        if o in ('-D', '--debug'):
            level = logging.DEBUG

    umobj_logging(level)

    logging.info("Using server %s" % server)
    logging.info("Running %s" % sys.argv)

    if access_key is None:
        try:
            access_key = os.environ['OBJ_ACCESS_KEY_ID']
        except:
            logging.error("Please provide access_key")
            usage()
    if secret_key is None:
        try:
            secret_key = os.environ['OBJ_SECRET_ACCESS_KEY']
        except:
            logging.error("Please provide secret_key")
            usage()

    obj = S3Connection(host=server, port=port, is_secure=True,
                       aws_access_key_id=access_key,
                       aws_secret_access_key=secret_key,
                       calling_format=OrdinaryCallingFormat())

    if len(args) < 2:
        usage()

    src = args[0]
    dest = args[-1]
    upload = False
    download = False
    src_match = re.match('([\w\.\-]+):([\w\.\-\/]*)', src)
    dest_match = re.match('([\w\.\-]+):([\w\.\-\/]*)', dest)

    if dest_match:
        if src_match:
            if not os.path.exists(src):
                logging.error("Can not copy from a bucket to a bucket.")
                usage()
        bucket_name = dest_match.group(1)
        dest_name = dest_match.group(2)
        upload = True
        logging.info("Upload mode with bucket %s" % bucket_name)
        if dest_name:
             logging.info("Uploading to the prefix %s." % dest_name)
        logging.info("Uploading files %s." % args[0:-1])
    else:
        if src_match:
            if len(args) > 2:
                logging.error("Uploads only support a single BUCKET:SRC and DEST.")
                usage()
            bucket_name = src_match.group(1)
            key_name = src_match.group(2)
            download = True
            logging.info("Download mode with bucket %s." % bucket_name)
            if key_name:
                logging.info("Downloading with the key name/prefix %s." % key_name)
            logging.info("Downloading to the local directory %s." % dest)
        else:
            logging.error("You need to provide a bucket in either SRC " +
                          "or DEST.")
            usage()

    try:
        bucket = obj.get_bucket(bucket_name)
    except boto.exception.S3ResponseError, e:
        logging.error("Unable to access bucket %s, %s." %
                      (bucket_name, e.error_code))
        sys.exit()

    if upload:
        for source in args[0:-1]:
            cpobj_upload(access_key, secret_key, bucket_name, source, dest_name, recursive, multi)
    if download:
        cpobj_download(access_key, secret_key, bucket_name, dest, key_name, recursive, multi)
