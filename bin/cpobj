#!/usr/bin/env python

import os
import sys
import math
import logging
import boto.s3
from boto.s3.connection import S3Connection
from boto.s3.connection import OrdinaryCallingFormat
import progressbar
import mimetypes
import threading
import Queue
import time
from multiprocessing import Pool
from filechunkio import FileChunkIO

pbar = None

usage_string = """
SYNOPSIS
   cpobj [-a/--access_key <access_key>] [-s/--secret_key <secret_key>]
         [-S/--server <server>] [-P/--port <port>] 
         [-m/--multipart] [-r/--recursive] [-h/--help]
         <bucket> <file/directory>+

   Where 
        access_key  - Your Access Key ID.  If not supplied, boto will
                      use the value of the environment variable
                      OBJ_ACCESS_KEY_ID
        secret_key  - Your Secret Access Key.  If not supplied, boto
                      will use the value of the environment variable
                      OBJ_SECRET_ACCESS_KEY
        bucket_name - The name of the bucket to list the files.  If not 
                      supplied it will just list the buckets.

        bucket      - Bucket to upload to
        file/dir    - File(s) or Directories(s) to be uploaded.  You will
                      need to specify the recursive flag to upload 
                      directories.
"""

def usage():
    print usage_string
    sys.exit()

def transfer_stats(trans_bytes,total_bytes):
    #print '%d bytes transferred / %d bytes total' % (trans_bytes, total_bytes)
    #sys.stdout.write('\r[{0}] {1}%'.format('#'*(progress/10), progress))
    try:
        pbar.update(trans_bytes)
    except AssertionError, e:
        print e

def upload_file(key,filename):

    global pbar 

    file_size = os.stat(filename).st_size
    print "INFO: Uploading %s with %d bytes" % (filename,file_size)
    pbar = progressbar.ProgressBar(maxval=file_size)
    pbar.start()
    try:
        key.set_contents_from_filename(filename,cb=transfer_stats,num_cb=100)
    except IOError, e:
        print e
        return 0
    pbar.finish()
    return file_size


class UploadThread(threading.Thread):
    def __init__(self, mp, queue):
        threading.Thread.__init__(self)
        self.mp = mp
        self.queue = queue
        self.obj = self.mp.connect()

    def run(self):
        while True:
            try:
                part_num, offset, bytes = self.queue.get(True, 2)
                logging.info('Starting part %d on offset %d with %d bytes.' % (part_num, offset, bytes))
            except Queue.Empty:
                return
            self._upload_part(part_num, offset, bytes)
            self.queue.task_done()

    def _upload_part(self, part_num, offset, bytes, retries=10):
        try:
            bucket = self.obj.get_bucket(self.mp.bucketname)
            for mp in bucket.get_all_multipart_uploads():
                if mp.id == self.mp.mp_id:
                    logging.info('Uploading chunk (%d) %s' % (retries, part_num))
                    with FileChunkIO(self.mp.filename, 'r', offset=offset,
                        bytes=bytes) as fp:
                        mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
        except Exception, exc:
            print exc
            if retries:
                self._upload_part(part_num, offset,
                                  bytes, retries=retries-1)
            else:
                logging.info('Failed uploading part %d' % part_num)
                raise exc
        else:
            logging.info('Uploaded part %d' % part_num)
 

class MultiPart:

    ### code adapted from https://gist.github.com/fabiant7t/924094
    def __init__(self,access_key,secret_key,server='obj.umiacs.umd.edu',
                 port=443,secure=True):
        self.access_key = access_key
        self.secret_key = secret_key
        self.server = server
        self.port = port
        self.secure = secure
        self.mp_id = None
        self.bucketname = None
        self.filename = None

    def connect(self):
        return S3Connection(host=self.server,
                            port=self.port,
                            is_secure=self.secure,
                            aws_access_key_id=self.access_key,
                            aws_secret_access_key=self.secret_key,
                            calling_format=OrdinaryCallingFormat())

    def start(self,bucketname,keyname,filename,threads=4,acl='private'):
        self.bucketname = bucketname
        self.filename = filename
        headers = {}
        obj = self.connect()
        bucket = obj.get_bucket(self.bucketname)
        mtype = mimetypes.guess_type(keyname)[0] or 'application/octet-stream'
        headers.update({'Content-Type': mtype}) 
        mp = bucket.initiate_multipart_upload(keyname, headers=headers) 
        self.mp_id = mp.id
        source_size = os.stat(filename).st_size
        bytes_per_chunk = max(int(math.sqrt(5242880) * math.sqrt(source_size)),
                              5242880)
        chunk_amount = int(math.ceil(source_size / float(bytes_per_chunk)))
        #print "Size: %16d   Chunk Size: %16d   Number Chunks: %8d" % (source_size, bytes_per_chunk, chunk_amount) 
        queue = Queue.Queue()
        #print "Starting a pool for %s with %d threads." % (mp.id, threads)

        ### putting all the tasks together in a queue
        for i in range(chunk_amount):
            offset = i * bytes_per_chunk
            remaining_bytes = source_size - offset
            bytes = min([bytes_per_chunk, remaining_bytes])
            part_num = i + 1
            queue.put((part_num, offset, bytes))

        for i in range(threads):
            t = UploadThread(self, queue)
            t.setDaemon(True)
            t.start()

        pbar = progressbar.ProgressBar(maxval=chunk_amount)
        pbar.start()
        while not queue.empty():
            pbar.update(chunk_amount - queue.qsize())
            time.sleep(1)
        pbar.finish()
        queue.join()
 
        if len(mp.get_all_parts()) == chunk_amount:
            mp.complete_upload()
            key = bucket.get_key(keyname)
            key.set_acl('private')
        else:
            print "Canceling upload..."
            mp.cancel_upload()

if __name__ == "__main__":
    import getopt

    try:
       opts, args = getopt.getopt(
                       sys.argv[1:], 'a:b:s:S:P:hmr',
                       ['access_key=', 'secret_key=','server=', 'port=',
                       'multipart', 'help', 'recursive']
                    )
    except getopt.GetoptError, err:
      print str(err) # will print something like "option -a not recognized"
      usage()
 

    multi = False
    access_key = None
    secret_key = None
    bucket_name = None
    try:
       server = os.environ['OBJ_SERVER']    
    except:
       server = 'obj.umiacs.umd.edu'
    port = 443    
    recursive = False

    for o,a in opts:
       if o in ('-h', '--help'):
          usage()
          sys.exit()
       if o in ('a', '--access_key'):
          access_key = a
       if o in ('s', '--secret_key'):
          secret_key = a
       if o in ('b', '--bucket'):
          bucket_name = a
       if o in ('S', '--server'):
          server = a
       if o in ('P', '--port'):
          port = a
       if o in ('m', '--multipart'):
          multi = True
       if o in ('r', '--recursive'):
          recursive = True

    if access_key == None:
       try:
          access_key = os.environ['OBJ_ACCESS_KEY_ID']
       except:
          print " ERROR: Please provide access_key"
          usage()
    if secret_key == None:
       try:
          secret_key = os.environ['OBJ_SECRET_ACCESS_KEY']
       except:
          print " ERROR: Please provide secret_key"
          usage()

    obj = S3Connection(host=server,
   		       port=port,
    		       is_secure=True,
		       aws_access_key_id=access_key,
                       aws_secret_access_key=secret_key,
		       calling_format=OrdinaryCallingFormat())

    if len(args) < 2:
        usage() 

    bucket_name = args[0]
    try:
        bucket = obj.get_bucket(bucket_name)
    except boto.exception.S3ResponseError:
        print "ERROR: bucket %s not found." % bucket_name
        sys.exit()
    
    if recursive:
        for d in args[1:]:
            if not os.path.isdir(d):
                print "ERROR: %s is not a directory to syncronize." % d
                continue
            for root, dirs, files in os.walk(d.rstrip(os.sep)):
                for f in files:    
                    filename = root + os.sep + f
                    keyname = filename.lstrip(os.sep)
                    key = bucket.new_key(keyname)
                    upload_file(key,filename)
    else:
        for f in args[1:]:
            if not os.path.isfile(f):
                print "ERROR: file %s does not exist." % f
                continue
            key_name = f.lstrip('.').lstrip('/').rstrip('/')
            if multi:
                m = MultiPart(access_key,secret_key,server,port)
                m.start(bucket_name,key_name,f)
            else:
                key = bucket.new_key(key_name)
                upload_file(key,f)
