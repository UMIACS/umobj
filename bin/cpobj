#!/usr/bin/env python

import os
import sys
import math
import re
import logging
import boto.s3
from boto.s3.connection import S3Connection
from boto.s3.connection import OrdinaryCallingFormat
import progressbar
import mimetypes
import threading
import Queue
import time
from filechunkio import FileChunkIO

pbar = None


def usage():
    prog_name = os.path.basename(sys.argv[0])
    usage_string = """
NAME
    %s - Copy files to and from a S3 compatiable object store

SYNOPSIS
    %s [OPTION]... SRC BUCKET[:DEST]

    %s [OPTION]... BUCKET[:SRC] DEST

OPTIONS SUMMARY

    -a, --access_key <access_key>  Object store access key
    -s, --secret_key <secret_key>  Object store secret key
    -S, --server <server>          Object store server name
    -P, --port <port>              Object store server port [Default: 443]
    -m, --multipart                Instantiate multipart uploads [Default: no]
    -r, --recursive                Copy files recursively [Default: no]
    -f, --force                    Force overwrite files in download mode
    -h, --help

OPTIONS
    access_key  - Your Access Key ID.  If not supplied, boto will
                  use the value of the environment variable OBJ_ACCESS_KEY_ID.
    secret_key  - Your Secret Access Key.  If not supplied, will use the value
                  of the environment variable OBJ_SECRET_ACCESS_KEY.
    server      - The object storage server you want to connect to.  This can
                  be overridden by the OBJ_SERVER environment variable.
    BUCKET      - The bucket that is to be used for this copy operation.  This
                  is necessary on one and only one of the SRC and DEST
                  arguments.
    SRC         - Local file system directory/file or object store key.  A
                  directory is allowed for uploads if recursive is turned on,
                  and is allowed for download.
    DEST        - Local file system directory/file or object store key.  A
                  directory is allowed for downloads.

""" % (prog_name, prog_name, prog_name)
    print usage_string
    sys.exit()


def transfer_stats(trans_bytes, total_bytes):
    try:
        pbar.update(trans_bytes)
    except AssertionError, e:
        print e


def upload_file(key, filename):
    global pbar
    file_size = os.stat(filename).st_size
    print "INFO: Uploading %s with %d bytes." % (filename, file_size)
    pbar = progressbar.ProgressBar(maxval=file_size)
    pbar.start()
    try:
        key.set_contents_from_filename(filename, cb=transfer_stats, num_cb=100)
    except IOError, e:
        print e
        return 0
    pbar.finish()
    return file_size


def download_file(key, filename):
    global pbar
    print "INFO: Downloading %s with %d bytes." % (filename, key.size)
    pbar = progressbar.ProgressBar(maxval=key.size)
    pbar.start()
    if os.path.isdir(filename):
        filename = filename + os.sep + key.name
    f = open(filename, 'w')
    key.get_contents_to_file(f, cb=transfer_stats, num_cb=100)
    f.close()
    pbar.finish()


class UploadThread(threading.Thread):
    def __init__(self, mp, queue):
        threading.Thread.__init__(self)
        self.mp = mp
        self.queue = queue
        self.obj = self.mp.connect()

    def run(self):
        while True:
            try:
                part_num, offset, bytes = self.queue.get(True, 2)
                logging.info('Starting part %d on offset %d with %d bytes.' %
                             (part_num, offset, bytes))
            except Queue.Empty:
                return
            self._upload_part(part_num, offset, bytes)
            self.queue.task_done()

    def _upload_part(self, part_num, offset, bytes, retries=10):
        try:
            bucket = self.obj.get_bucket(self.mp.bucketname)
            for mp in bucket.get_all_multipart_uploads():
                if mp.id == self.mp.mp_id:
                    logging.info('Uploading chunk (%d) %s' %
                                 (retries, part_num))
                    with FileChunkIO(self.mp.filename, 'r', offset=offset,
                                     bytes=bytes) as fp:
                        mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
        except Exception, exc:
            print exc
            if retries:
                self._upload_part(part_num, offset,
                                  bytes, retries=retries-1)
            else:
                logging.info('Failed uploading part %d' % part_num)
                raise exc
        else:
            logging.info('Uploaded part %d' % part_num)


class DownloadThread(threading.Thread):
    def __init__(self, mp, queue):
        threading.Thread.__init__(self)
        self.mp = mp
        self.queue = queue
        self.obj = self.mp.connect()

    def run(self):
        while True:
            try:
                start_byte, end_byte = self.queue.get(True, 2)
                logging.info('Starting download bytes %d - %d.' %
                             (start_byte, end_byte))
            except Queue.Empty:
                return
            self._download_part(start_byte, end_byte)
            self.queue.task_done()

    def _download_part(self, start_byte, end_byte):
        try:
            key_range = self.obj.make_request('GET',
                                              bucket=self.mp.bucketname,
                                              key=self.mp.keyname,
                                              headers={'Range': "bytes=%d-%d" %
                                              (start_byte, end_byte)})
            fd = os.open(self.mp.filename, os.O_WRONLY)
            logging.info("Opening file descriptor %d, seeking to %d" %
                         (fd, start_byte))
            os.lseek(fd, start_byte, os.SEEK_SET)
            chunk_size = min((end_byte-start_byte), 32 * 1024 * 1024)
            while True:
                data = key_range.read(chunk_size)
                if data == "":
                    break
                os.write(fd, data)
        except Exception, exc:
            print exc


class MultiPart:

    ### code adapted from https://gist.github.com/fabiant7t/924094
    def __init__(self, access_key, secret_key, server='obj.umiacs.umd.edu',
                 port=443, secure=True):
        self.access_key = access_key
        self.secret_key = secret_key
        self.server = server
        self.port = port
        self.secure = secure
        self.mp_id = None
        self.bucketname = None
        self.filename = None
        self.keyname = None

    def connect(self):
        return S3Connection(host=self.server,
                            port=self.port,
                            is_secure=self.secure,
                            aws_access_key_id=self.access_key,
                            aws_secret_access_key=self.secret_key,
                            calling_format=OrdinaryCallingFormat())

    def start_download(self, bucketname, keyname, filename, threads=4):
        self.bucketname = bucketname
        self.filename = filename
        obj = self.connect()
        bucket = obj.get_bucket(self.bucketname)
        key = bucket.get_key(keyname)
        size = key.size
        if size < 5 * 1024 * 1024:
            # don't multipart < 5MB files
            download_file(key, filename)
            return
        else:
            # create the file
            fd = os.open(self.filename, os.O_CREAT)
            os.close(fd)
            bytes_per_chunk = max(int(math.sqrt(5242880) * math.sqrt(size)),
                                  5242880)
            chunk_amount = int(math.ceil(size / float(bytes_per_chunk)))
            queue = Queue.Queue()
            for i in range(chunk_amount):
                offset = i * bytes_per_chunk
                remaining_bytes = size - offset
                bytes = min([bytes_per_chunk, remaining_bytes])
                queue.put((offset, offset + bytes - 1))
            for i in range(threads):
                t = DownloadThread(self, queue)
                t.setDaemon(True)
                t.start()
            queue.join()

    def start_upload(self, bucketname, keyname, filename, threads=4,
                     acl='private'):
        self.bucketname = bucketname
        self.filename = filename
        headers = {}
        obj = self.connect()
        bucket = obj.get_bucket(self.bucketname)
        mtype = mimetypes.guess_type(keyname)[0] or 'application/octet-stream'
        headers.update({'Content-Type': mtype})
        mp = bucket.initiate_multipart_upload(keyname, headers=headers)
        self.mp_id = mp.id
        source_size = os.stat(filename).st_size
        bytes_per_chunk = max(int(math.sqrt(5242880) * math.sqrt(source_size)),
                              5242880)
        chunk_amount = int(math.ceil(source_size / float(bytes_per_chunk)))
        #print "Size: %16d   Chunk Size: %16d   Number Chunks: %8d" %
        #        (source_size, bytes_per_chunk, chunk_amount)
        queue = Queue.Queue()
        #print "Starting a pool for %s with %d threads." % (mp.id, threads)

        ### putting all the tasks together in a queue
        for i in range(chunk_amount):
            offset = i * bytes_per_chunk
            remaining_bytes = source_size - offset
            bytes = min([bytes_per_chunk, remaining_bytes])
            part_num = i + 1
            queue.put((part_num, offset, bytes))

        for i in range(threads):
            t = UploadThread(self, queue)
            t.setDaemon(True)
            t.start()

        pbar = progressbar.ProgressBar(maxval=chunk_amount)
        pbar.start()
        while not queue.empty():
            pbar.update(chunk_amount - queue.qsize())
            time.sleep(1)
        pbar.finish()
        queue.join()

        if len(mp.get_all_parts()) == chunk_amount:
            mp.complete_upload()
            key = bucket.get_key(keyname)
            key.set_acl('private')
        else:
            print "Canceling upload..."
            mp.cancel_upload()

if __name__ == "__main__":
    import getopt

    try:
        opts, args = getopt.getopt(sys.argv[1:],
                                   'a:b:s:S:P:hmrf',
                                   ['access_key=', 'secret_key=',
                                   'server=', 'port=', 'multipart',
                                   'help', 'recursive', 'force'])
    except getopt.GetoptError, err:
        print str(err)  # will print something like "option -a not recognized"
        usage()

    force = False
    multi = False
    access_key = None
    secret_key = None
    bucket_name = None
    try:
        server = os.environ['OBJ_SERVER']
    except:
        server = 'obj.umiacs.umd.edu'
    port = 443
    recursive = False

    for o, a in opts:
        if o in ('-h', '--help'):
            usage()
            sys.exit()
        if o in ('a', '--access_key'):
            access_key = a
        if o in ('s', '--secret_key'):
            secret_key = a
        if o in ('b', '--bucket'):
            bucket_name = a
        if o in ('S', '--server'):
            server = a
        if o in ('P', '--port'):
            port = a
        if o in ('m', '--multipart'):
            multi = True
        if o in ('r', '--recursive'):
            recursive = True
        if o in ('f', '--force'):
            force = True

    if access_key is None:
        try:
            access_key = os.environ['OBJ_ACCESS_KEY_ID']
        except:
            print " ERROR: Please provide access_key"
            usage()
    if secret_key is None:
        try:
            secret_key = os.environ['OBJ_SECRET_ACCESS_KEY']
        except:
            print " ERROR: Please provide secret_key"
            usage()

    obj = S3Connection(host=server, port=port, is_secure=True,
                       aws_access_key_id=access_key,
                       aws_secret_access_key=secret_key,
                       calling_format=OrdinaryCallingFormat())

    if len(args) < 2:
        usage()

    src = args[0]
    dest = args[1]
    upload = False
    download = False
    src_match = re.match('(\w+):([\w\.\-\/]*)', src)
    dest_match = re.match('(\w+):([\w\.\-\/]*)', dest)

    if src_match:
        if dest_match:
            print "ERROR: Can not copy from a bucket to a bucket."
            usage()
        else:
            bucket_name = src_match.group(1)
            key_name = src_match.group(2)
            download = True
    else:
        if dest_match:
            bucket_name = dest_match.group(1)
            key_name = dest_match.group(2)
            upload = True
        else:
            print "ERROR: You need to provide a bucket in either SRC or DEST."
            usage()

    try:
        bucket = obj.get_bucket(bucket_name)
    except boto.exception.S3ResponseError:
        print "ERROR: bucket %s not found." % bucket_name
        sys.exit()

    if upload:
        if recursive:
            if not os.path.isdir(src):
                print "ERROR: SRC %s is not a directory." % src
            else:
                for root, dirs, files in os.walk(src.rstrip(os.sep)):
                    for f in files:
                        filename = root + os.sep + src
                        keyname = filename.lstrip(os.sep)
                        key = bucket.new_key(keyname)
                        upload_file(key, filename)
        else:
            if not os.path.isfile(src):
                print "ERROR: file %s does not exist." % src
                sys.exit(1)
            key_name = src.lstrip('.').lstrip('/').rstrip('/')
            if multi:
                m = MultiPart(access_key, secret_key, server, port)
                m.start_upload(bucket_name, key_name, f)
            else:
                key = bucket.new_key(key_name)
                upload_file(key, src)
    if download:
        if recursive:
            if not os.path.isdir(dest):
                print "ERROR: DEST %s is not a directory." % dest
            else:
                pass
        else:
            if os.path.isfile(dest) and not force:
                print "ERROR: file %s already exists " % dest + \
		      "please force flag to overwrite."
                usage()
            else:
                if multi:
                    m = MultiPart(access_key, secret_key, server, port)
                    m.start_download(bucket_name, key_name, dest)
                else:
                    key = bucket.get_key(key_name)
                    download_file(key, dest)
